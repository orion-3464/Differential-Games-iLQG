\subsection{The iLQG Algorithm} \label{ilqg_algo}

The iLQG algorithm is a method for solving general-sum differential non-linear games based on iterative Linear Quadratic Regulator (iLQR). The main idea is the iterative solution of approximate LQ games around a local trajectory. 

\subsubsection{Problem Formulation}

Based on \cite{fridovichkeil2020} the N-player finite horizon, general-sum differential game we focus on and will experiment with is defined by non-linear dynamics

$$\dot{x} = f(t, x, u_1, ..., u_N)$$

\noindent and cost functionals

$$J_i(u_1, ..., u_N) = \int_{0}^{T}g_i(t, x(t), u_1, ..., u_N)dt$$

\noindent where $x \in \mathbb{R}^n$ is the system state and $u_i \in \mathbb{R}^{m_i}$ is the control input of player $i$. \\
\noindent A strategy $\gamma_i$ is a function that determines what player $i$ is going to play any given moment in $t \in [0, T]$ according to their surroundings. 

$$u_i(t) = \gamma_i(t, x(t))$$

\noindent Namely, a strategy is a function $\gamma_i : [0, T] \times \mathbb{R}^n \rightarrow \mathbb{R}^{m_i}$. Our objective is to compute optimal feedback Nash equilibrium strategies $\gamma_i^{*}$, such that no player can reduce their cost $J_i$ by unilaterally deviating from their strategy. Formally, this is expressed by the inequalities: 

$$
J_i(\gamma_1^{*}; ...; \gamma_{i-1}^{*}; \gamma_i^{*}; \gamma_{i+1}^{*}; ...; \gamma_N^{*}) \leq 
J_i(\gamma_1^{*}; ...; \gamma_{i-1}^{*}; \gamma_i; \gamma_{i+1}^{*}; ...; \gamma_N^{*}), \; 
\forall i \in \{1, ..., N\} 
$$

\noindent Finding a global Nash equilibrium in a non-linear system is computationally intractable. Therefore, we adopt the iterative Linear Quadratic (iLQ) approach, which repeatedly constructs local linear-quadratic approximations of the dynamics and costs, enabling efficient numerical solution of the game in the neighborhood of a nominal trajectory.

\subsubsection{Local Approximation}

This section describes the initialization and the first steps of each iteration of the algorithm proposed in \cite{fridovichkeil2020}. \\

\noindent Given an initial feedback strategy $\{\gamma_i^0\}$ for each player $i$, and an initial state $x(0)$, the algorithm performs forward integration and yields a reference trajectory $\xi^k$. This trajectory describes the evolution of both the system and the players for $t \in [0, T]$. Specifically, it is defined as:

$$\xi^k = \{\hat{x}(t), \hat{u}_{1:N}(t)\}$$

\noindent where

\begin{itemize}
	\item $\hat{x}(t)$ is the trajectory of system's state
	
	\item $\hat{u}_{1:N}(t)$ are the control inputs of the players $i = 1, ..., N$ every moment.
\end{itemize}

\noindent This forward integration is implemented in three steps:

\begin{enumerate}
	\item Initialization $x(0)$
	
	\item At each time instant $t$, the players determine their control inputs according to the feedback law $\hat{u}_i(t) = \gamma_i^k(t, x(t))$.
	
	\item We solve the differential equation $\dot{x} = f(x, u_{1:N}, t)$ using a numerical method (here Runge-Kutta).
\end{enumerate}

\noindent After the initialization, the algorithm's next step is to obtain a Jacobian linearization of the dynamics $f$ about trajectory $\xi^k$ \cite{MurrayCDS101_2002}.

$$\dot{\delta}x(t) \approx A(t)\delta x(t) + \sum_{i\in [N]}B_i(t)\delta u_i(t)$$

\noindent where $A(t)$ is the Jacobian $D_{\hat{x}}f(t, \hat{x}(t), \hat{u}_{1:N}(t))$ and $B_i(t)$ is the Jacobian $D_{\hat{u}_i}f(t, \hat{x}(t), \hat{u}_{1:N}(t))$

\noindent A quadratic approximation of the quadratic cost is also obtained:

$$
g_i(t, x(t), u_{1:N}(t)) \approx 
g_i(t, \hat{x}(t), \hat{u}_{1:N}(t)) + 
\frac{1}{2}\delta x(t)^T(Q_i(t)\delta x(t) + 2l_i(t)) +
$$
$$
+ \frac{1}{2} \sum_{j \in [N]}\delta u_j(t)^T(R_{ij}(t)\delta u_j(t) + 2r_{ij}(t))
$$

\noindent where $l_i(t) = \nabla_{\hat{x}}g_i, \; r_{ij} = \nabla_{\hat{u}_{j}}g_i$ and matrices $Q_i$ and $R_{ij}$ are Hessians $D_{\hat{x}\hat{x}}^2g_i$ and $D_{\hat{u}_j\hat{u}_j}^2g_i$. The mixed partials (Hessians) are omitted because they rarely appear in cost functions of practical interest. 
$$

\noindent The linearized dynamics and the quadratic approximation of the cost together constitute a Linearâ€“Quadratic (LQ) game, analogous to the formulation described in the background section. Consequently, its solution can be obtained by solving a system of coupled Riccati differential equations, as presented in \cite{basar1999}. \\
\noindent After each iteration, we update our proposed strategies:

$$
 \gamma_i^k(t, x(t)) = \hat{u}_i(t) - P_i^k(t)\delta x(t) - \eta \alpha_i^k(t) 
 $$
 
 \noindent where $P_i^k$ is the solution of the coupled Riccati differential equation at iteration $k$ (LQ games' Nash equilibrium feedback control was $u_i^{*} = R_{ii}^{-1}B_i^T P_ix$) and $\alpha_i^k$ is an affine term related to minimization of Hamiltonian $\left(\frac{\partial H_i}{\partial u_i}\right)$. Parameter $\eta$ is a step size that we choose performing line search. Its purpose is to improve the algorithm's convergence.