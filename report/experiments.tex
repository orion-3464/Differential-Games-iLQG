In the previous section we proved that our algorithm is functional and reproduced a reduced result from paper \cite{fridovichkeil2020}. Now we designed 2 more experiments to demonstrate the effect of Nash equilibria on our players' strategies and examine the robustness of our method.

\subsection{Human-Robot Game 1}
We consider a three player game. One of the players is a robot (blue agent) and the other two are human (red and green). Our players' dynamics are described by the following model:

$$
\dot{p}_{x,i} = v_i \cos{\theta_i}, \;
\dot{p}_{y,i} = v_i \sin{\theta_i}, \;
\dot{u}_i = \alpha_i, \;
\dot{\theta}_i = \omega_i
$$

\noindent and our control input is:

$$
u_i = [\alpha_i, \omega_i]
$$

\noindent The players' initial positions are:

$$
p_{0, 1} = [-3, -3], \;
p_{0, 2} = [-3, 3], \;
p_{0, 3} = [3, -3]
$$

\noindent and their goal positions are:

$$
p_{g, 1} = [0, 0], \;
p_{g, 2} = [3, -3], \;
p_{g, 3} = [-3, 3]
$$

\noindent The cost function penalize distance from the target and proximity to other players. Specifically our cost functions are (for every player):

\begin{itemize}
	\item $\mathds{1}\{d_{ij} < d_{prox}\}(d_{prox}-d_{ij})^2$
	
	\item $d_{goal}^2$
	
	\item $u_i^T R u_i$
\end{itemize}

\noindent where $d_{ij}$ is the distance between two different players and $d_{goal}$ is the distance between the player and their goal. We are using a proximity threshold, $d_{prox}=0.5$. \\ 

\noindent Suppose that we linearize the dynamics and solve a classic optimal time control problem. Our solution is given by the bang-bang principle. Since every player's distance from the center is the same, they would collide. Our algorithm, utilizes the proximity cost and the Nash equilibrium calculation results to the avoidance of the collision. In fact, the robot slows down waiting both human to cross paths (without colliding) and then it reaches its destination. Here is a static plot of our results:

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{../simulation/results/robot_human1.png}
	\caption{Robot-human experiment 1}
\end{figure}  

\subsection{Human-Robot Game 2}
We confirmed that iLQG algorithm converges for two different scenarios. In the second one, we demonstrated the potential of our method. Now we are going to examine its robustness. \\
\noindent In this experiment, the players are characterized by identical dynamics and costs. However, at three random time instances, three random additional accelerations are introduced, distinct from the pre-calculated control input. The experiment's result is the following:

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{../simulation/results/robot_human2.png}
	\caption{Robot-human experiment 2}
\end{figure}  

\noindent This scenario showcases that iLQG can handle cases were there are unpredictable actions from the players. It computes different appropriate control inputs and manages to converge to a new Nash equilibrium despite the receding time horizon.