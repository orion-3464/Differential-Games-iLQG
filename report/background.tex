\subsection{Background} \label{back_tools}

\subsubsection{LQR Control Problem} \label{lqr_problem}
The Linear Quadratic Regulator problem is an optimal control problem concerning dynamic systems with linear dynamics and quadratic cost functions. The objective is to find the optimal control u that minimizes the cost J \cite{liberzon2012calculus}. 

\begin{itemize}
	\item A linear time-varying system:
			$$\dot{x} = A(t)x + B(t)u, \; x(t_0) = x_0, \;
			x \in \mathbb{R}^n, \; u \in \mathbb{R}^m$$
			
	\item Quadratic cost function:
			$$J(x, u, t) = \int_{t_0}^{t_f}(x^T(t)Qx(t) + u^T(t)Ru(t))dt + x^T(t_f)Q_fx(t_f),$$
			
			$$Q=Q^T \geq 0, \; Q_f=Q_f^T \geq 0, \; R=R^T > 0$$
			
	The running cost's matrices Q and R penalize the size of the state and the control effort respectively.
\end{itemize} 

\noindent We derive the necessary conditions for the minimization of J by applying Pontryagin's Maximum Principle. This yields that the optimal control is a linear state feedback law of the form:

$$u^{*}(t) = -R^{-1}(t)B^T(t)P(t)x^*(t)$$

\noindent where $P$ must be a solution of the matrix differential equation (Riccati differential equation (RDE)):

$$\dot{P}(t) = -P(t)A(t) - A^T(t)P(t)-Q(t) + P(t)B(t)R^{-1}(t)B^T(t)P(t), \; P(t_f) = Q_f$$ 

\noindent A special case of the LQR problem described above is the infinite-horizon LQR problem. Now, we assume that A, B, Q and R are constant (transient phenomena have subsided) and as $t_f \rightarrow \infty$ the terminal cost becomes negligible, thus $Q_f = 0$. So, the dynamics of the control system become time-invariant and the cost function becomes $J(x, u, t) = \int_{t_0}^{\infty}(x^T(t)Qx(t) + u^T(t)Ru(t))dt$. The optimal solution is given by:

\begin{enumerate}
	\item $u^{*}(t) = -R^{-1}B^TPx^{*}(t)$
	
	\item The limit $P = \lim_{t_f \rightarrow \infty}P(t_0, t_f)$ of the solution of the RDE is a constant matrix that satisfies the Algebraic Riccati Equation (ARE): 
			$$PA + A^TP + Q - PBR^{-1}B^TP = 0$$
			
	\item The optimal cost is: $J(u^{*}) = x_0^T P x_0$
	
	\item The closed-loop system $\dot{x}^{*} = (A-BR^{-1}B^TP)x^{*}$ is exponentially stable, as long as the couple $(A, \sqrt{Q})$ is detectable.
\end{enumerate}

\noindent The theoretical foundation of the LQR problem is necessary for the understanding of dynamic environments featuring multiple agents. In classical optimal control, our objective is to minimize a unique cost function. In contrast, in multi-player games, the optimization process is coupled. Actions of one agent influence the perfrormance and the state of the others. This transforms the Riccati equation to a system of coupled Riccati equations. Their solutions lead to a Nash equilibrium point, not a minimum. This framework is the basis of the iLQG algorithm which utilizes LQR iteratively by approximating non-linear dynamics as LQ problems.  

\subsubsection{General Sum Differential Games} \label{dif_games}

Before developing a theoretical framework in the context of general-sum differential games, it is advisable to examine several simpler cases. \\

\noindent \textbf{Zero-sum Matrix Games} \label{zero_sum} \\

\noindent To analyze multi-agent interactions, we first examine two-player zero-sum games, which are effectively represented in bi-matrix form \cite{basar1999}. Let $P_1$ and $P_2$ be the participants, choosing from m and n available strategies, respectively. The game is defined by a pair of matrices $(A, B)$, where each entry $(a_{ij} = J_1(u_1, u_2), b_{ij} = J_2(u_1, u_2))$ represents the cost incurred by $P_1$ and $P_2$ for a specific strategy pair $(i,j)$. In a zero-sum game, the players' interests are diametrically opposed, satisfying the condition $B=-A$. Consequently, for every outcome, the sum of their costs is $J_1(u_1, u_2) + J_2(u_1, u_2) = 0$, for every strategy pair $(u_1, u_2)$ implying that any advantage gained by one agent results in an equivalent loss for the other. 

\begin{table}[h!]
	\centering
	\begin{subtable}{0.45\textwidth}
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			 $P_1 \text{/} P_2$ & $a$ & $b$ \\
			\hline
				$c$ & $2, \; -2$ & $0, \; 0$ \\
			\hline
			$d$ & $4, \; -4$ & $-4, \; 4$ \\
			\hline
		\end{tabular}
		\caption{Game 1}
		\label{tab:example1_1}
	\end{subtable}
	\hfill
	\begin{subtable}{0.45\textwidth}
		\centering
		\begin{tabular}{|c|c|c|}
			\hline
			$P_1 \text{/} P_2$ & $a$ & $b$ \\
			\hline
			$c$ & $-2, \; 2$ & $0, \; 0$ \\
			\hline
			$d$ & $2, \; -2$ & $-2, \; 2$ \\
			\hline
		\end{tabular}
		\caption{Game 2}
		\label{tab:example1_2}
	\end{subtable}
	\caption{Examples of zero-sum games}
	\label{tab:example1}
\end{table}

\noindent In table \ref{tab:example1} above, matrices $A$ and $B$ are merged into a single cost matrix $C$. We suppose that $P_1$ can play either $c$ or $d$ and $P_2$ can play either a or b. Both $P_1$ and $P_2$ try to minimize their cost functions. \\
\noindent In the game in table \ref{tab:example1_1}, $P_2$ should rationally play strategy $a$ because it yields a lower cost for $P_2$​ regardless of what$P_1$ plays. Knowing $P_2$ is rational and will play $a$, $P_1$ plays strategy $c$ every time, choosing the lesser cost. Even if $P_2$ were not guaranteed to play $a$, $P_1$'s security strategy is to play $c$ because that is the strategy that ensures the smallest cost (2). This "rational" solution is called a \textit{saddle-point} solution. \\
In our second example (\ref{tab:example1_2}), there is no saddle-point solution. $P_1$'s security strategy is to play $c$ (this strategy minimizes his maximum loss - 0). $P_2$'s security strategies are both $a$ and $b$ (same smallest maximum loss). However, if $P_2$ plays securely ($b$) and $P_1$ knows this, $P_1$ has and incentive to switch to $b$. Conversely, if $P_1$ plays $c$ (security strategy) and $P_2$ knows this, $P_2$ has to choose $b$ from his security strategies. Thus, unlike the first game, there is not a Nash equilibrium. \\
\noindent A formal definition of Nash equilibrium can be found in \cite{Starr1969NonzerosumDG}. \\

\noindent Suppose a game with $Ν$ players. If $J_1(u_1, ..., u_N), ..., J_N(u_1, ..., u_N)$ are their respective cost functions, then the strategy set $\{u_1^{*}, ..., u_N^{*}\}$ is a Nash equilibrium if for every $i=1, ..., N$:

$$J_i(u_1^{*}, ..., u_{i-1}^{*}, u_i, u_{i+1}^{*}, ..., u_N^{*}) \geq J_i(u_1^{*}, ..., u_N^{*})$$ 

\noindent According to \cite{Starr1969NonzerosumDG}, all Nash equilibria are equivalent in a zero-sum game. Thus, every Nash equilibrium's strategy set is interchangeable. \\
\noindent In zero-sum games, players’ interests are diametrically opposed—one’s gain is the other’s loss—so there is no possibility of mutual gain. Consequently, the primary solution concept of interest is a Nash (or minimax) equilibrium, where each player’s strategy is optimal given the other’s. \\

\noindent \textbf{Nonzero-sum Matrix Games} \\

\noindent In this case, the sum of the cost functions is not zero for every possible outcome dictated by the strategies selected. 

\begin{table}[h!]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		$P_1 \text{/} P_2$ & $\text{Not Betray}$ & $\text{Betray}$ \\
		\hline
		$\text{Not Betray}$ & $2, \; 2$ & $10, \; 1$ \\
		\hline
		$\text{Betray}$ & $1, \; 10$ & $5, \; 5$ \\
		\hline
	\end{tabular}
	\caption{Prisoner's Dilemma}
	\label{tab:prisoner}
\end{table}

\noindent Table \ref{tab:prisoner} is the famous game Prisoner's Dilemma in bi-matrix form. Here, the cost functions correspond to years in prison for each player ()$P_1$ and $P_2$). The only Nash equilibrium solution is $(u_1, u_2) = (\text{Betray}, \text{Betray})$. But, it is not optimal in the sense that $(u_1, u_2) = (\text{Not Betray}, \text{Not Betray})$ yields a better result for both players. This illustrates the possibility of mutual interest in nonzero-sum games. A solution produced by the coalition of all the players is called \text{pareto-optimal} solution \cite{Starr1969NonzerosumDG}. \\
\noindent In addition, in nonzero-sum games, every Nash equilibrium strategy is not equivalent to every other (cost-wise). \\

\noindent While the preceding discussion of zero-sum and nonzero-sum games provides a foundational understanding of strategic interaction in discrete-time, finite-action settings, many real-world problems involve dynamics, continuous time, and evolving states. To model strategic decision-making in such contexts—where players’ actions influence not only immediate payoffs but also future states and opportunities—we extend the framework to differential games. These games generalize optimal control theory to multi-agent settings, incorporating differential equations to describe state evolution and enabling the analysis of continuous-time strategic behavior over a horizon. In what follows, we introduce the core formulation of differential games, including dynamics, cost functionals, information structures, and solution concepts such as open-loop and feedback Nash equilibria. \\

\noindent \textbf{General Differential Games} \\

\noindent In General Differential Games \cite{Starr1969NonzerosumDG}, the formulation resembles that of classical optimal control problems. I a nonzero-sum, N-player differential game, each player $i$ seeks to select a strategy (control) $u_i$ so as to minimize an individual cost function of the form:

$$J_i = K_i(x(t_f), t_f) + \int_{0}^{t_f}L_i(x, u_1, ..., u_N)dt$$

\noindent The optimization's constraints are defined by the game's dynamics:

$$\dot{x} = f(x, u_1, ..., u_N, t), \; x(t_0) = x_0$$

\noindent Such a problem may include holonomic and inequality constraints on the state $x$ and controls $u_i$.

\noindent The players' strategies interact through both the dynamics and the cost functionals, leading to a strategic dynamic optimization problem. \\

\noindent A key distinction from classical optimal control problems is that, in differential games, one must specify the solution concept that characterizes rational strategic interaction among players. Unlike optimal control where a single decision-maker minimizes a cost functional, a differential game requires equilibria such as a Nash equilibrium or outcomes that are Pareto-optimal, depending on whether the setting is non-cooperative or cooperative \cite{Starr1969NonzerosumDG}. Here, it is assumed that every player knows the value of the state vector, the system parameters and its cost functions, but not the rival players' strategies. \\

\noindent This project is about a special class of differential games where the system is linear and the cost functions are quadratic. These are known as Linear Quadratic Games. Due to their structural simplicity, LQ games often admit closed-form or numerically efficient solutions, while still capturing fundamental trade-offs such as control effort versus state regulation and strategic interaction among players. \\

\noindent \textbf{Linear Quadratic Games} \\

\noindent The cost function of each player $i$ is of the form:

$$J_i = \frac{1}{2}(x^TS_{if}x)_{t=t_f} + \frac{1}{2}\int_{t_0}^{t_f}(x^TQx+\sum_{j=1}^{N}u_j^TR_{ij}u_j)dt$$ 

\noindent Player $i$ should choose a control strategy $u_i = \Psi_i(x, t)$ to minimize that cost function subject to the constraints imposed by the system's dynamics:

$$\dot{x} = Ax + \sum_{j=1}^{N}B_ju_j$$ 

\begin{itemize}
	\item \textit{Nash Equilibrium Solutions:} This type of solution is secure against arbitrary and attempts by single players to unilaterally alter their strategy. The reason lies withing Nash Equilibrium definition. One can observe that a player can only lose by deviating from his control strategy. Thus, those solutions are particularly interesting in games where cooperation is impossible or difficult to enforce.
	
	For our LQ game, the terminal and running cost for each player $i$ are:
	
	$$K_i(x(t_f), t_f) = \frac{1}{2}(x^TS_{if}x)_{t=t_f}$$
	
	$$L_i(x, u_1, ..., u_N) = x^TQx + \sum_{j=1}^{N}u_j^TR_{ij}u_j$$
	
	One can obtain the necessary conditions for a Nash equilibrium solution by applying Pontryagin's maximum principle. Those are listed in \cite{Starr1969NonzerosumDG}:
	
	\begin{enumerate}
		\item $\dot{x} = f(x, u_1, ..., u_N, t), \; x(t_0) = x_0$
		
		\item $\dot{\lambda}_i^T = -\frac{\partial H_i(x, u_1, ..., u_N, t, \lambda_i)}{\partial x} - \sum_{j=1, j\neq i}^{N} \frac{\partial H_i}{\partial u_j} \frac{\partial \Psi_i}{\partial x}(x, t)$
		
		\item $\lambda_i^T(x(t_f)) = \frac{\partial K_i(x(t_f), t_f)}{x(t_f)}$
		
		\item $u_i = \Psi_i(x, t) = \arg\min\limits_{u_i^{'}} H_i(x, t, \Psi_1, ..., \Psi_{i-1}, u_i^{'}, \Psi_{i+1}, ..., \Psi_N, \lambda_i)$
	\end{enumerate}
	
	The Hamiltonian of each player in an LQ game is:
	
	$$
	H_i(x, t, u, \lambda_i^T) = L_i(x, u, t) + \lambda_i^Tf(x, u, t) =
	x^TQx + \sum_{j=1}^{N}u_j^TR_{ij}u_j + \lambda_i^T \left(Ax + \sum_{j=1}^{N}B_ju_j \right)
	$$
	
	The application of necessary condition 4 yields:
	
	$$\frac{\partial H_i}{\partial u_i} = 2R_{ii}u_i + B_i^T\lambda_i$$
	
	$$
	\frac{\partial H_i}{\partial u_i} = 0 \implies 
	u_i^{*} = -\frac{1}{2}R_{ii}^{-1}B_i^T\lambda_i$$
	
	Then, we apply condition 2 and get:
	
	$$u_i^{*} = R_{ii}^{-1}B_i^T P_ix$$
	
	where $S_i$ is a solution of the coupled Riccati equation:
	
	$$
	\dot{P}_i = -P_i A - A^TP_i - Q_i - \sum_{j=1}^{N}(P_jB_jR_{jj}^{-1}R_{ij}R_{jj}^{-1}B_j^TP_j-P_iB_jR_{jj}^{-1}B_j^TP_j-P_jB_jR_{jj}^{-1}B_j^TP_i)
	$$
	
	$$P_i(t_f) = P_{if}$$
	
	For $N=1$, this is the RDE equation that was presented in our LQR Control Problem section (corresponds to 1 player).
	
	\item \textit{Minimax Controls:} Paper \cite{Starr1969NonzerosumDG} states that this is the solution of a two-player, zero-sum LQ game where the opponent chooses strategy trying to maximize $J_i$. The minimax control for player $i$ is:
	
	$$\bar{u}_i = -R_{ii}^{-1}B_i^T\bar{P}_ix$$
	
	where
	
	$$\dot{\bar{P}}_i = -\bar{P}_iA-A^T\bar{P}_i-Q_i+\bar{P}_i\sum_{j=1}^{N}B_jR_{ij}^{-1}B_j^T\bar{P}_i, \; R_{ii} > 0, \; R_{ij} < 0, \; j\neq i$$
	
	This strategy's goal is to guarantee minimum cost against the worst possible set of strategies the other players can choose. It shall be used when one is not sure if their opponents will play their Nash controls.
	
	\newpage
	
	\item \textit{Non-inferior Controls:} This type of solutions are preferable if a negotiated solution can be reached and enforced. For LQ games the non-inferior controls are:
	
	$$\hat{u}_i(\mu) = -\left[\sum_{j=1}^{N}\mu_jR_{ji}\right]^{-1}B_i^T\hat{P}(\mu)x$$
	
	where
	
	$$\dot{\hat{P}}(\mu) = -\hat{P}A - A^T\hat{P} - \sum_{j=1}^{N}Q_j + \hat{P} \sum_{i=1}^{N}B_i\left[\sum_{j=1}^{N}\mu_jR_{ji}\right]B_i^T\hat{P}, \; \hat{P}(\mu, t_f) = \sum_{i=1}^{N}\mu_i P_{if}$$
	
	$$\sum_{j=1}^{N}\mu_i = 1, \mu_i \geq 0$$
	
\end{itemize}

\subsection{The iLQG Algorithm} \label{ilqg_algo}
